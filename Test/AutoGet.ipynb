{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87384119",
   "metadata": {},
   "source": [
    "这个文件是对md文件进行抽取的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0af3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pymysql\n",
    "from py2neo import Graph, Node, Relationship\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "import time\n",
    "\n",
    "class NSTIKnowledgeExtractor:\n",
    "    def __init__(self, mysql_config: Dict, neo4j_config: Dict, ollama_host: str = \"http://localhost:11434\", ollama_model: str = \"deepseek-r1:1.5b\"):\n",
    "        self.mysql_config = mysql_config\n",
    "        self.neo4j_config = neo4j_config\n",
    "        self.ollama_host = ollama_host\n",
    "        self.ollama_model = ollama_model\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # 初始化数据库连接\n",
    "        self.mysql_conn = None\n",
    "        self.neo4j_graph = None\n",
    "        self.connect_databases()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def connect_databases(self):\n",
    "        \"\"\"连接MySQL和Neo4j数据库\"\"\"\n",
    "        try:\n",
    "            # 连接MySQL\n",
    "            self.mysql_conn = pymysql.connect(**self.mysql_config)\n",
    "            self.logger.info(\"MySQL连接成功\")\n",
    "            \n",
    "            # 连接Neo4j\n",
    "            self.neo4j_graph = Graph(**self.neo4j_config)\n",
    "            self.logger.info(\"Neo4j连接成功\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"数据库连接失败: {e}\")\n",
    "    \n",
    "    def check_ollama_health(self) -> bool:\n",
    "        \"\"\"检查Ollama服务是否健康\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "        except ImportError:\n",
    "            self.logger.error(\"请安装requests库: pip install requests\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            response = requests.get(f\"{self.ollama_host}/api/tags\", timeout=2)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def chunk_text(self, text: str, chunk_size: int = 3000) -> List[str]:\n",
    "        \"\"\"将长文本分割成适合模型处理的块\"\"\"\n",
    "        # 按段落分割\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            if len(current_chunk) + len(paragraph) < chunk_size:\n",
    "                current_chunk += paragraph + \"\\n\\n\"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph + \"\\n\\n\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def call_ollama_with_retry(self, prompt: str, max_retries: int = 3) -> str:\n",
    "        \"\"\"带重试机制的Ollama调用\"\"\"\n",
    "        # 先检查Ollama服务是否可用\n",
    "        if not self.check_ollama_health():\n",
    "            self.logger.error(\"Ollama服务不可用，请检查是否启动\")\n",
    "            return \"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = ollama.generate(model=self.ollama_model, prompt=prompt)\n",
    "                return response['response']\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Ollama调用失败 (尝试 {attempt + 1}/{max_retries}): {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        self.logger.error(\"Ollama调用完全失败\")\n",
    "        return \"\"\n",
    "    \n",
    "    def extract_knowledge_from_chunk(self, chunk: str, chunk_id: int) -> Dict:\n",
    "        \"\"\"从文本块中提取知识\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        请从以下医学文献片段中提取结构化知识，严格按照JSON格式返回：\n",
    "\n",
    "        需要提取的实体和关系：\n",
    "\n",
    "        实体类型：\n",
    "        1. Disease(疾病): name(名称), type(类型), description(描述), prevalence(患病率), mortality(病死率)\n",
    "        2. Symptom(症状): name(名称), type(类型), description(描述), severity(严重程度)\n",
    "        3. DiagnosticMethod(诊断方法): name(名称), type(类型), description(描述), sensitivity(敏感度), specificity(特异度)\n",
    "        4. Treatment(治疗方法): name(名称), type(类型), description(描述), efficacy(效果)\n",
    "        5. Pathogen(病原体): name(名称), type(类型), description(描述), related_diseases(相关疾病)\n",
    "        6. Drug(药物): name(名称), type(类型), description(描述), indication(适应症)\n",
    "        7. Recommendation(推荐意见): content(内容), evidence_level(证据等级), recommendation_level(推荐等级)\n",
    "\n",
    "        关系类型：\n",
    "        1. HAS_SYMPTOM(有症状)\n",
    "        2. DIAGNOSED_BY(通过诊断)\n",
    "        3. TREATED_WITH(通过治疗)\n",
    "        4. CAUSED_BY(由病原体引起)\n",
    "        5. PRESCRIBED_DRUG(使用药物)\n",
    "        6. FOLLOWS_RECOMMENDATION(遵循推荐)\n",
    "\n",
    "        请返回如下JSON格式：\n",
    "        {{\n",
    "            \"entities\": {{\n",
    "                \"diseases\": [],\n",
    "                \"symptoms\": [],\n",
    "                \"diagnostic_methods\": [],\n",
    "                \"treatments\": [],\n",
    "                \"pathogens\": [],\n",
    "                \"drugs\": [],\n",
    "                \"recommendations\": []\n",
    "            }},\n",
    "            \"relationships\": []\n",
    "        }}\n",
    "\n",
    "        文本内容：\n",
    "        {chunk}\n",
    "\n",
    "        注意：只返回JSON格式，不要其他文字。\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.call_ollama_with_retry(prompt)\n",
    "        \n",
    "        try:\n",
    "            # 尝试从响应中提取JSON\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                return json.loads(json_match.group())\n",
    "            else:\n",
    "                self.logger.warning(f\"块 {chunk_id} 未找到有效JSON\")\n",
    "                return {\"entities\": {}, \"relationships\": []}\n",
    "        except json.JSONDecodeError as e:\n",
    "            self.logger.error(f\"块 {chunk_id} JSON解析失败: {e}\")\n",
    "            return {\"entities\": {}, \"relationships\": []}\n",
    "    \n",
    "    def extract_knowledge_from_markdown(self, md_file_path: str) -> Dict:\n",
    "        \"\"\"从Markdown文件提取知识\"\"\"\n",
    "        \n",
    "        # 读取Markdown文件\n",
    "        with open(md_file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 分割文本\n",
    "        chunks = self.chunk_text(content)\n",
    "        self.logger.info(f\"将文本分割为 {len(chunks)} 个块\")\n",
    "        \n",
    "        all_entities = {\n",
    "            \"diseases\": [],\n",
    "            \"symptoms\": [],\n",
    "            \"diagnostic_methods\": [],\n",
    "            \"treatments\": [],\n",
    "            \"pathogens\": [],\n",
    "            \"drugs\": [],\n",
    "            \"recommendations\": []\n",
    "        }\n",
    "        all_relationships = []\n",
    "        \n",
    "        # 处理每个块\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            self.logger.info(f\"处理块 {i+1}/{len(chunks)}\")\n",
    "            result = self.extract_knowledge_from_chunk(chunk, i+1)\n",
    "            \n",
    "            # 合并实体\n",
    "            for entity_type in all_entities.keys():\n",
    "                if entity_type in result.get(\"entities\", {}):\n",
    "                    all_entities[entity_type].extend(result[\"entities\"][entity_type])\n",
    "            \n",
    "            # 合并关系\n",
    "            if \"relationships\" in result:\n",
    "                all_relationships.extend(result[\"relationships\"])\n",
    "            \n",
    "            # 避免频繁调用\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return {\n",
    "            \"entities\": all_entities,\n",
    "            \"relationships\": all_relationships\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca0974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySQLManager:\n",
    "    def __init__(self, mysql_conn):\n",
    "        self.conn = mysql_conn\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.create_tables()\n",
    "    \n",
    "    def create_tables(self):\n",
    "        \"\"\"创建MySQL表结构\"\"\"\n",
    "        \n",
    "        tables = {\n",
    "            'diseases': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS diseases (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(255) NOT NULL,\n",
    "                    type VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    prevalence VARCHAR(100),\n",
    "                    mortality VARCHAR(100),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'symptoms': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS symptoms (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(255) NOT NULL,\n",
    "                    type VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    severity VARCHAR(50),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'diagnostic_methods': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS diagnostic_methods (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(255) NOT NULL,\n",
    "                    type VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    sensitivity VARCHAR(100),\n",
    "                    specificity VARCHAR(100),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'treatments': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS treatments (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(255) NOT NULL,\n",
    "                    type VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    efficacy TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'pathogens': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS pathogens (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(255) NOT NULL,\n",
    "                    type VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    related_diseases TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'drugs': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS drugs (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    name VARCHAR(255) NOT NULL,\n",
    "                    type VARCHAR(100),\n",
    "                    description TEXT,\n",
    "                    indication TEXT,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'recommendations': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS recommendations (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    content TEXT NOT NULL,\n",
    "                    evidence_level VARCHAR(50),\n",
    "                    recommendation_level VARCHAR(50),\n",
    "                    consistency_rate VARCHAR(50),\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\",\n",
    "            'relationships': \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS relationships (\n",
    "                    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                    source_type VARCHAR(50) NOT NULL,\n",
    "                    source_id INT NOT NULL,\n",
    "                    target_type VARCHAR(50) NOT NULL,\n",
    "                    target_id INT NOT NULL,\n",
    "                    relationship_type VARCHAR(100) NOT NULL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        for table_name, create_sql in tables.items():\n",
    "            try:\n",
    "                self.cursor.execute(create_sql)\n",
    "                self.conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"创建表 {table_name} 失败: {e}\")\n",
    "    \n",
    "    def save_entities_to_mysql(self, entities: Dict):\n",
    "        \"\"\"保存实体到MySQL\"\"\"\n",
    "        \n",
    "        for entity_type, entity_list in entities.items():\n",
    "            if not entity_list:\n",
    "                continue\n",
    "                \n",
    "            for entity in entity_list:\n",
    "                # 构建插入SQL\n",
    "                columns = ', '.join(entity.keys())\n",
    "                placeholders = ', '.join(['%s'] * len(entity))\n",
    "                sql = f\"INSERT INTO {entity_type} ({columns}) VALUES ({placeholders})\"\n",
    "                \n",
    "                try:\n",
    "                    self.cursor.execute(sql, list(entity.values()))\n",
    "                    self.conn.commit()\n",
    "                except Exception as e:\n",
    "                    print(f\"插入{entity_type}失败: {e}\")\n",
    "                    self.conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5401ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jManager:\n",
    "    def __init__(self, neo4j_graph):\n",
    "        self.graph = neo4j_graph\n",
    "    \n",
    "    def clear_database(self):\n",
    "        \"\"\"清空图数据库\"\"\"\n",
    "        self.graph.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    \n",
    "    def create_entities_in_neo4j(self, entities: Dict):\n",
    "        \"\"\"在Neo4j中创建实体节点\"\"\"\n",
    "        \n",
    "        node_mapping = {}  # 存储节点ID映射\n",
    "        \n",
    "        for entity_type, entity_list in entities.items():\n",
    "            for entity in entity_list:\n",
    "                # 创建节点\n",
    "                node_properties = {k: v for k, v in entity.items() if v is not None}\n",
    "                node = Node(entity_type.upper(), **node_properties)\n",
    "                self.graph.create(node)\n",
    "                \n",
    "                # 存储节点映射\n",
    "                if entity_type not in node_mapping:\n",
    "                    node_mapping[entity_type] = {}\n",
    "                node_mapping[entity_type][entity['name']] = node\n",
    "        \n",
    "        return node_mapping\n",
    "    \n",
    "    def create_relationships_in_neo4j(self, relationships: List, node_mapping: Dict):\n",
    "        \"\"\"在Neo4j中创建关系\"\"\"\n",
    "        \n",
    "        for rel in relationships:\n",
    "            try:\n",
    "                source_node = node_mapping[rel['source_type']][rel['source_name']]\n",
    "                target_node = node_mapping[rel['target_type']][rel['target_name']]\n",
    "                \n",
    "                relationship = Relationship(source_node, rel['type'], target_node)\n",
    "                self.graph.create(relationship)\n",
    "            except KeyError as e:\n",
    "                print(f\"创建关系失败，找不到节点: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a09fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:37:40,348 - INFO - MySQL连接成功\n",
      "2025-10-06 23:37:40,361 - INFO - Neo4j连接成功\n",
      "2025-10-06 23:37:40,365 - INFO - 将文本分割为 7 个块\n",
      "2025-10-06 23:37:40,366 - INFO - 处理块 1/7\n",
      "2025-10-06 23:37:40,383 - WARNING - Ollama调用失败 (尝试 1/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:42,394 - WARNING - Ollama调用失败 (尝试 2/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:44,403 - WARNING - Ollama调用失败 (尝试 3/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:46,406 - ERROR - Ollama调用完全失败\n",
      "2025-10-06 23:37:46,408 - WARNING - 块 1 未找到有效JSON\n",
      "2025-10-06 23:37:47,413 - INFO - 处理块 2/7\n",
      "2025-10-06 23:37:47,424 - WARNING - Ollama调用失败 (尝试 1/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:49,436 - WARNING - Ollama调用失败 (尝试 2/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:51,444 - WARNING - Ollama调用失败 (尝试 3/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:53,446 - ERROR - Ollama调用完全失败\n",
      "2025-10-06 23:37:53,446 - WARNING - 块 2 未找到有效JSON\n",
      "2025-10-06 23:37:54,450 - INFO - 处理块 3/7\n",
      "2025-10-06 23:37:54,470 - WARNING - Ollama调用失败 (尝试 1/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:56,475 - WARNING - Ollama调用失败 (尝试 2/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:37:58,478 - WARNING - Ollama调用失败 (尝试 3/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:38:00,485 - ERROR - Ollama调用完全失败\n",
      "2025-10-06 23:38:00,485 - WARNING - 块 3 未找到有效JSON\n",
      "2025-10-06 23:38:01,492 - INFO - 处理块 4/7\n",
      "2025-10-06 23:38:01,512 - WARNING - Ollama调用失败 (尝试 1/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "2025-10-06 23:38:03,518 - WARNING - Ollama调用失败 (尝试 2/3): Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 数据库配置\n",
    "    mysql_config = {\n",
    "        'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': '123456',\n",
    "        'database': 'doctor',\n",
    "        'charset': 'utf8mb4'\n",
    "    }\n",
    "    \n",
    "    neo4j_config = {\n",
    "        'uri': \"bolt://localhost:7687\",\n",
    "        'auth': (\"neo4j\", \"test1234\")\n",
    "    }\n",
    "    \n",
    "    # 初始化提取器\n",
    "    extractor = NSTIKnowledgeExtractor(mysql_config, neo4j_config)\n",
    "    \n",
    "    # 提取知识\n",
    "    md_file_path = r\"O:\\MyProject\\RAG\\DB\\uploads\\output.md\"  # 您的OCR转换文件\n",
    "    knowledge_data = extractor.extract_knowledge_from_markdown(md_file_path)\n",
    "    \n",
    "    # 保存到MySQL\n",
    "    mysql_manager = MySQLManager(extractor.mysql_conn)\n",
    "    mysql_manager.save_entities_to_mysql(knowledge_data[\"entities\"])\n",
    "    \n",
    "    # 保存到Neo4j\n",
    "    neo4j_manager = Neo4jManager(extractor.neo4j_graph)\n",
    "    neo4j_manager.clear_database()\n",
    "    node_mapping = neo4j_manager.create_entities_in_neo4j(knowledge_data[\"entities\"])\n",
    "    neo4j_manager.create_relationships_in_neo4j(knowledge_data[\"relationships\"], node_mapping)\n",
    "    \n",
    "    print(\"知识抽取和存储完成！\")\n",
    "    \n",
    "    # 关闭连接\n",
    "    if extractor.mysql_conn:\n",
    "        extractor.mysql_conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dd79d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
