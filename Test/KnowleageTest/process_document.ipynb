{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HTML文档处理流程\n",
        "## 任务：\n",
        "1. 复制HTML文件，命名为title内容（去除特殊字符）\n",
        "2. 处理HTML内容\n",
        "3. 转换为markdown格式\n",
        "4. 提取p标签内容并按句号分割\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import html2text\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "原始标题: 坏死性软组织感染临床诊治急诊专家共识\n",
            "清理后的标题: 坏死性软组织感染临床诊治急诊专家共识\n"
          ]
        }
      ],
      "source": [
        "# 读取原始HTML文件\n",
        "input_file = 'output_document.html'\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "# 解析HTML\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# 获取title内容\n",
        "title = soup.find('title').text\n",
        "print(f\"原始标题: {title}\")\n",
        "\n",
        "# 去除特殊字符\n",
        "clean_title = re.sub(r'[\\\\/:*?\"<>|]', '', title)\n",
        "print(f\"清理后的标题: {clean_title}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已复制文件到: 坏死性软组织感染临床诊治急诊专家共识.html\n"
          ]
        }
      ],
      "source": [
        "# 步骤1: 复制文件，使用title作为文件名\n",
        "copy_filename = f\"{clean_title}.html\"\n",
        "shutil.copy(input_file, copy_filename)\n",
        "print(f\"已复制文件到: {copy_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "找到DOI标签，位置: 6，内容: DOI: 10.3760/cma.j.issn.1671-0282.2023.11.007\n",
            "已删除前7个p标签\n"
          ]
        }
      ],
      "source": [
        "# 步骤2: 处理原文件内容\n",
        "# 重新解析用于处理的HTML\n",
        "soup_processing = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# 2.1 找到包含\"DOI:\"的p标签，删除该标签及其之前的所有p标签\n",
        "all_p_tags = soup_processing.find_all('p')\n",
        "doi_index = -1\n",
        "\n",
        "for i, p in enumerate(all_p_tags):\n",
        "    if 'DOI:' in p.get_text():\n",
        "        doi_index = i\n",
        "        print(f\"找到DOI标签，位置: {i}，内容: {p.get_text()[:50]}\")\n",
        "        break\n",
        "\n",
        "# 删除DOI及之前的所有p标签\n",
        "if doi_index >= 0:\n",
        "    for i in range(doi_index + 1):\n",
        "        all_p_tags[i].decompose()\n",
        "    print(f\"已删除前{doi_index + 1}个p标签\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "找到参考文献标签: h2, 原始内容: 参 考\t 文\t 献\n",
            "已删除参考文献及之后的内容\n"
          ]
        }
      ],
      "source": [
        "# 2.2 从文件末尾向前查找包含\"参考文献\"的内容，删除该内容之后的所有部分\n",
        "# 查找包含\"参考文献\"的标签（处理\"参 考 文 献\"这种带空格的情况）\n",
        "reference_tag = None\n",
        "for tag in soup_processing.find_all(['h1', 'h2', 'h3', 'p']):\n",
        "    # 去除所有空白字符后检查是否包含\"参考文献\"\n",
        "    clean_text = re.sub(r'\\s+', '', tag.get_text())\n",
        "    if '参考文献' in clean_text:\n",
        "        reference_tag = tag\n",
        "        print(f\"找到参考文献标签: {tag.name}, 原始内容: {tag.get_text()}\")\n",
        "        break\n",
        "\n",
        "# 删除参考文献标签及之后的所有内容\n",
        "if reference_tag:\n",
        "    # 获取该标签之后的所有兄弟标签并删除\n",
        "    for sibling in list(reference_tag.next_siblings):\n",
        "        if hasattr(sibling, 'decompose'):\n",
        "            sibling.decompose()\n",
        "    # 删除参考文献标签本身\n",
        "    reference_tag.decompose()\n",
        "    print(\"已删除参考文献及之后的内容\")\n",
        "else:\n",
        "    print(\"警告：未找到参考文献标签\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "找到5个table标签\n",
            "已删除所有table标签\n"
          ]
        }
      ],
      "source": [
        "# 2.3 删除所有table标签\n",
        "tables = soup_processing.find_all('table')\n",
        "print(f\"找到{len(tables)}个table标签\")\n",
        "for table in tables:\n",
        "    table.decompose()\n",
        "print(\"已删除所有table标签\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已处理73个p标签，去除空格、换行符和[]符号\n"
          ]
        }
      ],
      "source": [
        "# 2.4 去除所有p标签内部的空格、换行符以及[]符号及其内容\n",
        "p_tags = soup_processing.find_all('p')\n",
        "for p in p_tags:\n",
        "    # 获取文本内容\n",
        "    text = p.get_text()\n",
        "    # 先去除[]及其中的内容\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # 再去除空格和换行符\n",
        "    cleaned_text = re.sub(r'\\s+', '', text)\n",
        "    # 替换p标签的内容\n",
        "    p.string = cleaned_text\n",
        "\n",
        "print(f\"已处理{len(p_tags)}个p标签，去除空格、换行符和[]符号\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已保存markdown文件: ragt.md\n",
            "Markdown内容长度: 11814 字符\n"
          ]
        }
      ],
      "source": [
        "# 步骤3: 将处理后的HTML转换为markdown格式\n",
        "processed_html = str(soup_processing)\n",
        "\n",
        "# 使用html2text转换\n",
        "h = html2text.HTML2Text()\n",
        "h.ignore_links = False\n",
        "h.ignore_images = False\n",
        "h.body_width = 0  # 不限制宽度\n",
        "\n",
        "markdown_content = h.handle(processed_html)\n",
        "\n",
        "# 保存为ragt.md\n",
        "with open('ragt.md', 'w', encoding='utf-8') as f:\n",
        "    f.write(markdown_content)\n",
        "\n",
        "print(\"已保存markdown文件: ragt.md\")\n",
        "print(f\"Markdown内容长度: {len(markdown_content)} 字符\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已保存ner.txt文件，共73个p标签，每10个p标签一行\n",
            "总共8行\n",
            "\n",
            "前3个p标签内容示例:\n",
            "1. 坏死性软组织感染(necrotizingsofttissueinfections，NSTIs)是由病原微生物感染导致皮下组织、筋膜或（和）肌肉坏死的一类疾病。N...\n",
            "2. 本专家共识以软组织感染、坏死性软组织感染、坏死性筋膜炎为中文检索词，以Necrotizingfasciitis、Softtissueinfections、Nec...\n",
            "3. 据报道，每年皮肤软组织感染的急诊就诊人数约540~840万，其中12%~40%患者需住院治疗，0.7%需收住监护室。NSTIs发病率在(1.1~15.5)/10...\n"
          ]
        }
      ],
      "source": [
        "# 步骤4: 提取所有p标签内容，每十个p标签拼接成一行\n",
        "p_tags = soup_processing.find_all('p')\n",
        "\n",
        "# 提取所有p标签的文本内容\n",
        "p_texts = []\n",
        "for p in p_tags:\n",
        "    text = p.get_text()\n",
        "    if text.strip():  # 只处理非空文本\n",
        "        # 去除[]及其中的内容（如果之前步骤有遗漏）\n",
        "        text = re.sub(r'\\[.*?\\]', '', text)\n",
        "        text = text.strip()\n",
        "        if text:\n",
        "            p_texts.append(text)\n",
        "\n",
        "# 保存为ner.txt，每十个p标签拼接成一行\n",
        "with open('ner.txt', 'w', encoding='utf-8') as f:\n",
        "    for i in range(0, len(p_texts), 10):\n",
        "        # 获取当前批次的p标签内容（最多10个）\n",
        "        batch = p_texts[i:i+10]\n",
        "        # 用空格连接这些p标签内容，写入一行\n",
        "        f.write(' '.join(batch) + '\\n')\n",
        "\n",
        "print(f\"已保存ner.txt文件，共{len(p_texts)}个p标签，每10个p标签一行\")\n",
        "print(f\"总共{(len(p_texts) + 9) // 10}行\")\n",
        "print(\"\\n前3个p标签内容示例:\")\n",
        "for i, text in enumerate(p_texts[:3]):\n",
        "    print(f\"{i+1}. {text[:80]}...\" if len(text) > 80 else f\"{i+1}. {text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 处理完成 ===\n",
            "1. 已复制原文件为: 坏死性软组织感染临床诊治急诊专家共识.html\n",
            "2. 已删除DOI之前的7个p标签\n",
            "3. 已删除参考文献及之后的内容\n",
            "4. 已删除5个table标签\n",
            "5. 已去除所有p标签内的空格、换行符和[]符号\n",
            "6. 已保存markdown文件: ragt.md\n",
            "7. 已保存ner.txt文件（共73个p标签，每10个p标签一行）\n"
          ]
        }
      ],
      "source": [
        "# 总结输出\n",
        "print(\"\\n=== 处理完成 ===\")\n",
        "print(f\"1. 已复制原文件为: {copy_filename}\")\n",
        "print(f\"2. 已删除DOI之前的{doi_index + 1}个p标签\")\n",
        "print(f\"3. 已删除参考文献及之后的内容\")\n",
        "print(f\"4. 已删除{len(tables)}个table标签\")\n",
        "print(f\"5. 已去除所有p标签内的空格、换行符和[]符号\")\n",
        "print(f\"6. 已保存markdown文件: ragt.md\")\n",
        "print(f\"7. 已保存ner.txt文件（共{len(p_texts)}个p标签，每10个p标签一行）\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "My",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
