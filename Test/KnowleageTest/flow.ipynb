{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "# 定义Pydantic模型\n",
    "class ClassifiedEntities(BaseModel):\n",
    "    \"\"\"按类型分类的实体模型\"\"\"\n",
    "    type: str = Field(description=\"实体类型\")\n",
    "    entities: List[str] = Field(default_factory=list, description=\"该类型的所有实体\")\n",
    "    count: int = Field(description=\"该类型的实体数量\")\n",
    "\n",
    "def test_api_connection():\n",
    "    \"\"\"\n",
    "    测试API连接是否正常\n",
    "    \"\"\"\n",
    "    url = \"https://zjlchat-ner.vip.cpolar.cn/predict\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    test_text = \"测试文本\"\n",
    "    data = {\"text\": test_text}\n",
    "    \n",
    "    print(\"正在测试API连接...\")\n",
    "    print(f\"API地址: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, headers=headers, timeout=10)\n",
    "        print(f\"状态码: {response.status_code}\")\n",
    "        print(f\"响应头: {dict(response.headers)}\")\n",
    "        print(f\"响应内容: {response.text[:200]}\")  # 只显示前200个字符\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"✓ API连接正常\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ API返回错误状态码: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ API连接失败: {e}\")\n",
    "        return False\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    将文本按句子分割\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # 按句号、问号、感叹号等分割，但保留标点符号\n",
    "    sentences = re.split(r'([。！？\\n])', text)\n",
    "    \n",
    "    # 重新组合句子和标点\n",
    "    result = []\n",
    "    for i in range(0, len(sentences)-1, 2):\n",
    "        if sentences[i].strip():  # 跳过空句子\n",
    "            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "            result.append(sentence.strip())\n",
    "    \n",
    "    # 处理最后一个可能没有标点的句子\n",
    "    if len(sentences) % 2 == 1 and sentences[-1].strip():\n",
    "        result.append(sentences[-1].strip())\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_ner_text():\n",
    "    \"\"\"\n",
    "    从ner.txt文件中抽取医学命名实体，为构建知识图谱做准备\n",
    "    \n",
    "    处理流程：\n",
    "    1. 读取ner.txt文件，拼接所有行后按句子分割\n",
    "    2. 将每个句子传入实体识别模型进行NER\n",
    "    3. 按实体类型分类并去重\n",
    "    4. 保存到JSON文件，供后续关系抽取和知识图谱构建使用\n",
    "    \n",
    "    实体类型映射到KG：\n",
    "    - dis(疾病) -> Disease\n",
    "    - sym(症状) -> Symptom  \n",
    "    - ite(检验项目) -> Test\n",
    "    - pro(操作/治疗) -> Treatment\n",
    "    - mic(微生物) -> Pathogen\n",
    "    - dru(药物) -> Drug\n",
    "    - bod(身体部位) -> BodyPart\n",
    "    \"\"\"\n",
    "    # 定义API地址\n",
    "    url = \"https://zjlchat-ner.vip.cpolar.cn/predict\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # 定义文件路径\n",
    "    ner_file_path = r\"O:\\MyProject\\Knowleage\\ner.txt\"\n",
    "    output_dir = r\"O:\\MyProject\\Knowleage\"\n",
    "    \n",
    "    # 存储所有结果\n",
    "    results = []\n",
    "    \n",
    "    # 读取ner.txt文件\n",
    "    try:\n",
    "        with open(ner_file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        print(f\"成功读取文件，共 {len(lines)} 行\")\n",
    "        \n",
    "        # 拼接所有行\n",
    "        full_text = ' '.join(line.strip() for line in lines if line.strip())\n",
    "        print(f\"拼接后文本总长度: {len(full_text)} 字符\")\n",
    "        \n",
    "        # 按句子分割\n",
    "        sentences = split_into_sentences(full_text)\n",
    "        print(f\"分割后共 {len(sentences)} 个句子\\n\")\n",
    "        \n",
    "        # 处理每个句子\n",
    "        for idx, sentence in enumerate(sentences, 1):\n",
    "            if not sentence:  # 跳过空句子\n",
    "                continue\n",
    "            \n",
    "            print(f\"正在处理第 {idx}/{len(sentences)} 句...\")\n",
    "            print(f\"  文本长度: {len(sentence)} 字符\")\n",
    "            if len(sentence) > 50:\n",
    "                print(f\"  内容预览: {sentence[:50]}...\")\n",
    "            \n",
    "            # 准备请求数据\n",
    "            data = {\"text\": sentence}\n",
    "            \n",
    "            try:\n",
    "                # 发送请求到实体识别API\n",
    "                response = requests.post(url, json=data, headers=headers, timeout=30)\n",
    "                \n",
    "                # 检查响应状态\n",
    "                if response.status_code == 405:\n",
    "                    print(f\"  ✗ 405错误：方法不允许\")\n",
    "                    results.append({\n",
    "                        \"text\": sentence,\n",
    "                        \"entities\": [],\n",
    "                        \"error\": \"405 Method Not Allowed\"\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # 解析JSON响应\n",
    "                result = response.json()\n",
    "                \n",
    "                # 简化实体信息，只保留实体名称和类型\n",
    "                simplified_entities = []\n",
    "                if \"entities\" in result and isinstance(result[\"entities\"], list):\n",
    "                    for entity in result[\"entities\"]:\n",
    "                        simplified_entities.append({\n",
    "                            \"entity\": entity.get(\"entity\", \"\"),\n",
    "                            \"type\": entity.get(\"type\", \"\")\n",
    "                        })\n",
    "                \n",
    "                # 保存简化后的结果\n",
    "                results.append({\n",
    "                    \"text\": sentence,\n",
    "                    \"entities\": simplified_entities\n",
    "                })\n",
    "                \n",
    "                print(f\"  ✓ 识别到 {len(simplified_entities)} 个实体\")\n",
    "                \n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"  ✗ 连接错误\")\n",
    "                results.append({\n",
    "                    \"text\": sentence,\n",
    "                    \"entities\": [],\n",
    "                    \"error\": \"连接错误\"\n",
    "                })\n",
    "            except requests.exceptions.Timeout as e:\n",
    "                print(f\"  ✗ 请求超时\")\n",
    "                results.append({\n",
    "                    \"text\": sentence,\n",
    "                    \"entities\": [],\n",
    "                    \"error\": \"请求超时\"\n",
    "                })\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                print(f\"  ✗ HTTP错误\")\n",
    "                results.append({\n",
    "                    \"text\": sentence,\n",
    "                    \"entities\": [],\n",
    "                    \"error\": \"HTTP错误\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ 其他错误: {e}\")\n",
    "                results.append({\n",
    "                    \"text\": sentence,\n",
    "                    \"entities\": [],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # 使用Pydantic模型处理结果 - 按类型收集实体\n",
    "        entity_type_dict = defaultdict(set)  # 使用set避免重复\n",
    "        total_entity_count = 0\n",
    "        \n",
    "        for result in results:\n",
    "            entities = result.get(\"entities\", [])\n",
    "            total_entity_count += len(entities)\n",
    "            \n",
    "            # 按类型收集实体\n",
    "            for entity in entities:\n",
    "                entity_type_dict[entity.get(\"type\", \"\")].add(entity.get(\"entity\", \"\"))\n",
    "        \n",
    "        # 创建分类后的实体列表\n",
    "        classified_list = []\n",
    "        for entity_type, entity_set in sorted(entity_type_dict.items()):\n",
    "            if entity_type:  # 跳过空类型\n",
    "                classified = ClassifiedEntities(\n",
    "                    type=entity_type,\n",
    "                    entities=sorted(list(entity_set)),  # 排序并去重\n",
    "                    count=len(entity_set)\n",
    "                )\n",
    "                classified_list.append(classified)\n",
    "        \n",
    "        # 显示所有识别到的实体类型\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"NER识别完成\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\n统计信息：\")\n",
    "        print(f\"  - 总句子数: {len(results)}\")\n",
    "        print(f\"  - 总实体数: {total_entity_count}\")\n",
    "        print(f\"  - 实体类型数: {len(classified_list)}\")\n",
    "        print(f\"\\n识别到的实体类型：\")\n",
    "        for classified in classified_list:\n",
    "            print(f\"  - {classified.type}: {classified.count} 个不同实体\")\n",
    "        \n",
    "        # 过滤：只保留 sym 和 dis\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"过滤实体类型：只保留症状和疾病\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        kept_types = ['sym', 'dis']\n",
    "        filtered_list = []\n",
    "        removed_count = 0\n",
    "        \n",
    "        for classified in classified_list:\n",
    "            if classified.type in kept_types:\n",
    "                filtered_list.append(classified)\n",
    "                print(f\"  ✓ 保留 {classified.type}: {classified.count} 个实体\")\n",
    "            else:\n",
    "                removed_count += classified.count\n",
    "                print(f\"  ✗ 移除 {classified.type}: {classified.count} 个实体\")\n",
    "        \n",
    "        # 生成输出文件\n",
    "        output_file = os.path.join(output_dir, \"ner_results_classified.json\")\n",
    "        \n",
    "        # 保存过滤后的结果\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(\n",
    "                [item.model_dump() for item in filtered_list],\n",
    "                f,\n",
    "                ensure_ascii=False,\n",
    "                indent=2\n",
    "            )\n",
    "        \n",
    "        # 显示最终结果\n",
    "        final_entity_count = sum(item.count for item in filtered_list)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"处理完成！\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"结果已保存到: {output_file}\")\n",
    "        print(f\"\\n最终统计：\")\n",
    "        print(f\"  - 总句子数: {len(results)}\")\n",
    "        print(f\"  - 识别实体数: {total_entity_count}\")\n",
    "        print(f\"  - 保留实体数: {final_entity_count}\")\n",
    "        print(f\"  - 移除实体数: {removed_count}\")\n",
    "        print(f\"  - 保留比例: {final_entity_count/total_entity_count*100:.1f}%\")\n",
    "        print(f\"\\n保存的实体类型：\")\n",
    "        for classified in filtered_list:\n",
    "            type_name = \"疾病\" if classified.type == \"dis\" else \"症状\"\n",
    "            print(f\"  - {type_name} ({classified.type}): {classified.count} 个不同实体\")\n",
    "        \n",
    "        return output_file, results\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 找不到文件 {ner_file_path}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时发生错误: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 先测试API连接\n",
    "print(\"=\" * 60)\n",
    "api_ok = test_api_connection()\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if api_ok:\n",
    "    print(\"\\nAPI测试通过，开始处理文件...\\n\")\n",
    "    output_file, results = process_ner_text()\n",
    "else:\n",
    "    print(\"\\n警告：API测试失败，但仍然尝试处理文件...\\n\")\n",
    "    output_file, results = process_ner_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6469db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# 配置LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen2.5:14b\",\n",
    "    base_url=\"https://zjlchat.vip.cpolar.cn/v1\",\n",
    "    api_key=\"EMPTY\",\n",
    "    temperature=0.1,\n",
    "    top_p=0.8\n",
    ")\n",
    "\n",
    "def clean_entities():\n",
    "    \"\"\"\n",
    "    清理ner_results_classified.json中的实体：\n",
    "    1. 去除重复实体\n",
    "    2. 使用LLM判断并剔除不正确的实体\n",
    "    3. 智能合并相似的疾病和症状\n",
    "    \"\"\"\n",
    "    classified_file = r\"O:\\MyProject\\Knowleage\\ner_results_classified.json\"\n",
    "    \n",
    "    try:\n",
    "        # 1. 读取原始数据\n",
    "        with open(classified_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"实体清理智能体启动\")\n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "        \n",
    "        # 1. 检查数据结构\n",
    "        if isinstance(data, dict) and 'entities' in data:\n",
    "            # 如果已经包含relationships，只处理entities部分\n",
    "            classified_entities = data['entities']\n",
    "            has_relationships = True\n",
    "        else:\n",
    "            # 原始格式\n",
    "            classified_entities = data\n",
    "            has_relationships = False\n",
    "        \n",
    "        print(f\"原始数据统计：\")\n",
    "        total_entities_before = 0\n",
    "        for entity_group in classified_entities:\n",
    "            count = entity_group['count']\n",
    "            total_entities_before += count\n",
    "            type_name = \"疾病\" if entity_group['type'] == \"dis\" else (\"症状\" if entity_group['type'] == \"sym\" else entity_group['type'])\n",
    "            print(f\"  - {type_name} ({entity_group['type']}): {count} 个\")\n",
    "        print(f\"  总计: {total_entities_before} 个实体\\n\")\n",
    "        \n",
    "        # 2. 去重处理\n",
    "        print(\"=\" * 60)\n",
    "        print(\"步骤 1: 去除重复实体\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        deduplicated_entities = []\n",
    "        duplicate_count = 0\n",
    "        \n",
    "        for entity_group in classified_entities:\n",
    "            entity_type = entity_group['type']\n",
    "            original_entities = entity_group['entities']\n",
    "            \n",
    "            # 使用set去重，保持原有顺序\n",
    "            seen = set()\n",
    "            unique_entities = []\n",
    "            for entity in original_entities:\n",
    "                entity_lower = entity.lower().strip()\n",
    "                if entity_lower not in seen:\n",
    "                    seen.add(entity_lower)\n",
    "                    unique_entities.append(entity.strip())\n",
    "                else:\n",
    "                    duplicate_count += 1\n",
    "            \n",
    "            deduplicated_entities.append({\n",
    "                'type': entity_type,\n",
    "                'entities': unique_entities,\n",
    "                'count': len(unique_entities)\n",
    "            })\n",
    "            \n",
    "            if len(original_entities) != len(unique_entities):\n",
    "                print(f\"  {entity_type}: {len(original_entities)} → {len(unique_entities)} (去除 {len(original_entities) - len(unique_entities)} 个)\")\n",
    "        \n",
    "        print(f\"\\n✓ 共去除 {duplicate_count} 个重复实体\\n\")\n",
    "        \n",
    "        # 3. 使用LLM进行智能清理\n",
    "        print(\"=\" * 60)\n",
    "        print(\"步骤 2: 使用LLM验证实体有效性\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"（这可能需要几分钟，请耐心等待）\\n\")\n",
    "        \n",
    "        cleaned_entities = []\n",
    "        total_removed = 0\n",
    "        \n",
    "        type_descriptions = {\n",
    "            'bod': '身体部位（如皮肤、筋膜、肌肉、器官等）',\n",
    "            'dep': '医院科室（如急诊科、外科、感染科等）',\n",
    "            'dis': '疾病名称（如感染、炎症、综合征等）',\n",
    "            'dru': '药物名称（如抗生素、激素、中成药等）',\n",
    "            'equ': '医疗设备（如呼吸机、监护仪等）',\n",
    "            'ite': '检查项目或指标（如血常规、C反应蛋白等）',\n",
    "            'mic': '微生物或病原体（如细菌、病毒等）',\n",
    "            'pro': '医疗操作或程序（如手术、清创、检查等）',\n",
    "            'sym': '症状或体征（如发热、疼痛、水肿等）'\n",
    "        }\n",
    "        \n",
    "        for entity_group in deduplicated_entities:\n",
    "            entity_type = entity_group['type']\n",
    "            entities = entity_group['entities']\n",
    "            type_desc = type_descriptions.get(entity_type, entity_type)\n",
    "            \n",
    "            print(f\"正在检查 {entity_type} 类型的 {len(entities)} 个实体...\")\n",
    "            \n",
    "            # 将实体分批处理（每批30个）\n",
    "            batch_size = 30\n",
    "            valid_entities = []\n",
    "            \n",
    "            for i in range(0, len(entities), batch_size):\n",
    "                batch = entities[i:i+batch_size]\n",
    "                \n",
    "                system_prompt = f\"\"\"你是一个医学实体验证专家。你的任务是检查给定的实体列表，判断哪些实体属于指定的类别，哪些不属于或明显错误。\n",
    "\n",
    "当前类别：{entity_type} - {type_desc}\n",
    "\n",
    "验证标准：\n",
    "1. 实体必须是完整的医学术语，不能是句子片段或无意义内容\n",
    "2. 实体必须明确属于当前类别\n",
    "3. 剔除明显的识别错误（如乱码、标点符号、数字串等）\n",
    "4. 保留缩写、专业术语、中英文混合的医学术语\n",
    "\n",
    "输出格式：只输出JSON数组，包含有效的实体名称\n",
    "[\"实体1\", \"实体2\", \"实体3\"]\n",
    "\n",
    "重要：直接输出JSON数组，不要任何解释或markdown格式。\"\"\"\n",
    "\n",
    "                user_prompt = f\"\"\"请检查以下实体列表，只保留属于\"{type_desc}\"类别的有效实体：\n",
    "\n",
    "{json.dumps(batch, ensure_ascii=False)}\n",
    "\n",
    "输出有效实体的JSON数组：\"\"\"\n",
    "\n",
    "                try:\n",
    "                    messages = [\n",
    "                        SystemMessage(content=system_prompt),\n",
    "                        HumanMessage(content=user_prompt)\n",
    "                    ]\n",
    "                    \n",
    "                    response = llm.invoke(messages)\n",
    "                    response_text = response.content.strip()\n",
    "                    \n",
    "                    # 提取JSON\n",
    "                    if '[' in response_text and ']' in response_text:\n",
    "                        start_idx = response_text.find('[')\n",
    "                        end_idx = response_text.rfind(']')\n",
    "                        json_text = response_text[start_idx:end_idx+1]\n",
    "                        \n",
    "                        batch_valid = json.loads(json_text)\n",
    "                        valid_entities.extend(batch_valid)\n",
    "                        \n",
    "                        removed = len(batch) - len(batch_valid)\n",
    "                        if removed > 0:\n",
    "                            print(f\"  批次 {i//batch_size + 1}: 保留 {len(batch_valid)}/{len(batch)} 个实体\")\n",
    "                    else:\n",
    "                        # 如果解析失败，保留所有实体\n",
    "                        print(f\"  批次 {i//batch_size + 1}: 解析失败，保留所有实体\")\n",
    "                        valid_entities.extend(batch)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  批次 {i//batch_size + 1}: 处理出错 ({e})，保留所有实体\")\n",
    "                    valid_entities.extend(batch)\n",
    "            \n",
    "            removed_count = len(entities) - len(valid_entities)\n",
    "            total_removed += removed_count\n",
    "            \n",
    "            cleaned_entities.append({\n",
    "                'type': entity_type,\n",
    "                'entities': sorted(list(set(valid_entities))),  # 再次去重并排序\n",
    "                'count': len(valid_entities)\n",
    "            })\n",
    "            \n",
    "            if removed_count > 0:\n",
    "                print(f\"  ✓ {entity_type}: {len(entities)} → {len(valid_entities)} (剔除 {removed_count} 个)\\n\")\n",
    "            else:\n",
    "                print(f\"  ✓ {entity_type}: 保持 {len(valid_entities)} 个\\n\")\n",
    "        \n",
    "        print(f\"✓ 共剔除 {total_removed} 个无效实体\\n\")\n",
    "        \n",
    "        # 4. 合并相似实体（专门针对 dis 和 sym）\n",
    "        print(\"=\" * 60)\n",
    "        print(\"步骤 3: 合并相似实体\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"（针对疾病和症状进行智能合并）\\n\")\n",
    "        \n",
    "        merged_entities = []\n",
    "        total_merged = 0\n",
    "        \n",
    "        for entity_group in cleaned_entities:\n",
    "            entity_type = entity_group['type']\n",
    "            entities = entity_group['entities']\n",
    "            \n",
    "            # 只对疾病和症状进行合并处理\n",
    "            if entity_type in ['dis', 'sym'] and len(entities) > 1:\n",
    "                print(f\"正在分析 {entity_type} 类型的 {len(entities)} 个实体...\")\n",
    "                \n",
    "                # 使用LLM进行智能合并\n",
    "                system_prompt = f\"\"\"你是医学实体标准化专家。任务是识别并合并语义相似或重复的医学实体。\n",
    "\n",
    "实体类型：{entity_type} - {'疾病' if entity_type == 'dis' else '症状'}\n",
    "\n",
    "合并规则：\n",
    "1. 语义完全相同的实体应合并（如\"发热\"和\"发烧\"）\n",
    "2. 一般概念和特殊概念可合并（如\"疼痛\"可包含\"剧烈疼痛\"）\n",
    "3. 带修饰词的实体可与核心概念合并（如\"糖尿病患者\"合并到\"糖尿病\"）\n",
    "4. 同一疾病的不同称呼应合并（如\"NSTIs\"和\"坏死性软组织感染\"）\n",
    "5. 保留最标准、最常用的名称作为合并后的名称\n",
    "6. 如果不确定是否应该合并，保持独立\n",
    "\n",
    "输出格式：JSON对象，包含合并映射和最终实体列表\n",
    "{{\n",
    "  \"merges\": [\n",
    "    {{\"original\": [\"实体1\", \"实体2\"], \"merged\": \"标准名称\", \"reason\": \"合并原因\"}},\n",
    "  ],\n",
    "  \"final_entities\": [\"标准实体1\", \"标准实体2\", ...]\n",
    "}}\n",
    "\n",
    "重要：直接输出JSON，不要markdown格式。\"\"\"\n",
    "\n",
    "                user_prompt = f\"\"\"请分析以下{len(entities)}个实体，识别并合并相似的实体：\n",
    "\n",
    "{json.dumps(entities, ensure_ascii=False)}\n",
    "\n",
    "输出合并结果的JSON：\"\"\"\n",
    "\n",
    "                try:\n",
    "                    messages = [\n",
    "                        SystemMessage(content=system_prompt),\n",
    "                        HumanMessage(content=user_prompt)\n",
    "                    ]\n",
    "                    \n",
    "                    response = llm.invoke(messages)\n",
    "                    response_text = response.content.strip()\n",
    "                    \n",
    "                    # 提取JSON\n",
    "                    if '{' in response_text and '}' in response_text:\n",
    "                        # 移除可能的markdown标记\n",
    "                        if '```json' in response_text:\n",
    "                            response_text = response_text.split('```json')[1].split('```')[0].strip()\n",
    "                        elif '```' in response_text:\n",
    "                            response_text = response_text.split('```')[1].split('```')[0].strip()\n",
    "                        \n",
    "                        start_idx = response_text.find('{')\n",
    "                        end_idx = response_text.rfind('}') + 1\n",
    "                        json_text = response_text[start_idx:end_idx]\n",
    "                        \n",
    "                        merge_result = json.loads(json_text)\n",
    "                        \n",
    "                        if 'merges' in merge_result and 'final_entities' in merge_result:\n",
    "                            merges = merge_result['merges']\n",
    "                            final = merge_result['final_entities']\n",
    "                            \n",
    "                            # 显示合并详情\n",
    "                            if merges:\n",
    "                                print(f\"  发现 {len(merges)} 组需要合并的实体：\")\n",
    "                                for merge in merges:\n",
    "                                    originals = merge.get('original', [])\n",
    "                                    merged = merge.get('merged', '')\n",
    "                                    reason = merge.get('reason', '')\n",
    "                                    if len(originals) > 1:\n",
    "                                        print(f\"    • {' + '.join(originals)} → {merged}\")\n",
    "                                        print(f\"      原因: {reason}\")\n",
    "                                        total_merged += len(originals) - 1\n",
    "                            \n",
    "                            merged_entities.append({\n",
    "                                'type': entity_type,\n",
    "                                'entities': sorted(final),\n",
    "                                'count': len(final)\n",
    "                            })\n",
    "                            \n",
    "                            reduction = len(entities) - len(final)\n",
    "                            if reduction > 0:\n",
    "                                print(f\"  ✓ {entity_type}: {len(entities)} → {len(final)} (合并 {reduction} 个)\\n\")\n",
    "                            else:\n",
    "                                print(f\"  ✓ {entity_type}: 保持 {len(final)} 个（无需合并）\\n\")\n",
    "                        else:\n",
    "                            # 解析失败，保持原样\n",
    "                            print(f\"  ⚠ {entity_type}: 解析失败，保持原样\\n\")\n",
    "                            merged_entities.append(entity_group)\n",
    "                    else:\n",
    "                        # 解析失败，保持原样\n",
    "                        print(f\"  ⚠ {entity_type}: 解析失败，保持原样\\n\")\n",
    "                        merged_entities.append(entity_group)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ {entity_type}: 处理出错 ({e})，保持原样\\n\")\n",
    "                    merged_entities.append(entity_group)\n",
    "            else:\n",
    "                # 其他类型不合并，直接保留\n",
    "                merged_entities.append(entity_group)\n",
    "        \n",
    "        if total_merged > 0:\n",
    "            print(f\"✓ 共合并 {total_merged} 个相似实体\\n\")\n",
    "        else:\n",
    "            print(f\"✓ 未发现需要合并的实体\\n\")\n",
    "        \n",
    "        # 使用合并后的数据\n",
    "        cleaned_entities = merged_entities\n",
    "        \n",
    "        # 5. 保存清理后的数据\n",
    "        print(\"=\" * 60)\n",
    "        print(\"保存清理结果\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if has_relationships:\n",
    "            # 保持原有的relationships数据\n",
    "            result_data = {\n",
    "                'entities': cleaned_entities,\n",
    "                'relationships': data.get('relationships', []),\n",
    "                'relationship_count': data.get('relationship_count', 0),\n",
    "                'source_text_length': data.get('source_text_length', 0)\n",
    "            }\n",
    "        else:\n",
    "            result_data = cleaned_entities\n",
    "        \n",
    "        with open(classified_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✓ 清理后的数据已保存到: {classified_file}\\n\")\n",
    "        \n",
    "        # 6. 显示清理统计\n",
    "        print(\"=\" * 60)\n",
    "        print(\"清理结果统计\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_entities_after = sum(eg['count'] for eg in cleaned_entities)\n",
    "        \n",
    "        print(f\"清理前总实体数: {total_entities_before}\")\n",
    "        print(f\"清理后总实体数: {total_entities_after}\")\n",
    "        print(f\"总共处理: {total_entities_before - total_entities_after} 个实体\")\n",
    "        print(f\"  - 剔除无效: {total_removed} 个\")\n",
    "        print(f\"  - 合并重复: {total_merged} 个\")\n",
    "        print(f\"保留率: {total_entities_after/total_entities_before*100:.1f}%\\n\")\n",
    "        \n",
    "        print(\"各类型实体统计：\")\n",
    "        for entity_group in cleaned_entities:\n",
    "            type_name = \"疾病\" if entity_group['type'] == \"dis\" else (\"症状\" if entity_group['type'] == \"sym\" else entity_group['type'])\n",
    "            print(f\"  - {type_name} ({entity_group['type']}): {entity_group['count']} 个\")\n",
    "        \n",
    "        # 7. 显示部分清理示例\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"清理后的实体样例\")\n",
    "        print(\"=\" * 60)\n",
    "        for entity_group in cleaned_entities:\n",
    "            entity_type = entity_group['type']\n",
    "            entities = entity_group['entities']\n",
    "            type_name = \"疾病\" if entity_type == \"dis\" else (\"症状\" if entity_type == \"sym\" else entity_type)\n",
    "            \n",
    "            # 对疾病和症状显示更多，其他类型显示5个\n",
    "            display_count = 10 if entity_type in ['dis', 'sym'] else 5\n",
    "            entities_sample = entities[:display_count]\n",
    "            \n",
    "            print(f\"\\n{type_name} ({entity_type}) - 共 {len(entities)} 个:\")\n",
    "            for entity in entities_sample:\n",
    "                print(f\"  • {entity}\")\n",
    "            if len(entities) > display_count:\n",
    "                print(f\"  ... 还有 {len(entities) - display_count} 个\")\n",
    "        \n",
    "        return cleaned_entities\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"✗ 错误: 找不到文件 - {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"✗ 处理过程中发生错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# 执行实体清理\n",
    "print(\"=\" * 60)\n",
    "print(\"智能实体清理任务\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n功能：\")\n",
    "print(\"  1. 去除重复实体\")\n",
    "print(\"  2. 使用LLM剔除无效实体\")\n",
    "print(\"  3. 智能合并相似的疾病和症状\")\n",
    "print(\"\\n开始处理...\\n\")\n",
    "\n",
    "result = clean_entities()\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ 实体清理完成！\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n完成的处理：\")\n",
    "    print(\"  ✓ 去重 - 移除完全重复的实体\")\n",
    "    print(\"  ✓ 验证 - 剔除无效或错误的实体\")\n",
    "    print(\"  ✓ 合并 - 整合语义相似的疾病和症状\")\n",
    "    print(\"\\n结果已保存到 ner_results_classified.json\")\n",
    "else:\n",
    "    print(\"\\n✗ 实体清理失败\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
